{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/pandas_logo.png\" align=\"right\" width=\"40%\">\n",
    "\n",
    "DataFrame\n",
    "==========\n",
    "\n",
    "The `dask.dataframe` module implements a blocked parallel `DataFrame` object that mimics a subset of the Pandas `DataFrame`. One dask `DataFrame` is comprised of several in-memory pandas `DataFrames` separated along the index. One operation on a dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "*  [Dask DataFrame documentation](http://dask.pydata.org/en/latest/dataframe.html)\n",
    "*  [Pandas documentation](http://pandas.pydata.org/)\n",
    "\n",
    "**Main Take-aways**\n",
    "\n",
    "1.  Dask.dataframe should be familiar to Pandas users\n",
    "2.  The index grows to include partitions, which are important for efficient queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We create artifical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from prep import accounts_csvs\n",
    "accounts_csvs(3, 5000000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filename = os.path.join('data', 'accounts.*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from CSVs and inspect the dask graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dask.dataframe.read_csv`\n",
    "\n",
    "This works just like `pandas.read_csv`, except on multiple csv files at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/accounts.*.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.DataFrame<from-de..., npartitions=6>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However unlike Pandas, operations on dask.dataframes don't trigger immediate computation, instead they add key-value pairs to an underlying dask graph.\n",
    "\n",
    "Some operations trigger direct computation, like `len` and `head`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.92 s, sys: 380 ms, total: 5.3 s\n",
      "Wall time: 2.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>names</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>374</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366</td>\n",
       "      <td>Victor</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>490</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>Michael</td>\n",
       "      <td>4006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    names  amount\n",
       "0  374   Xavier      53\n",
       "1  366   Victor     868\n",
       "2   12  Charlie     -60\n",
       "3  490  Charlie       4\n",
       "4  228  Michael    4006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.memory_usage = 'deep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Data columns (total 3 columns):\n",
      "id          int64\n",
      "names       object\n",
      "amount      int64\n",
      "dtypes: object(1), int64(2)\n",
      "memory usage: 343.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this compare to Pandas?\n",
    "\n",
    "#### Features and Size\n",
    "\n",
    "Pandas is more mature and fully featured than `dask.dataframe`.  If your data fits in memory then you should use Pandas.  The `dask.dataframe` module gives you a limited `pandas` experience when you operate on datasets that don't fit comfortably in memory.\n",
    "\n",
    "During this tutorial we provide a small dataset consisting of a few CSV files.  This dataset is 45MB on disk that expands to about 400MB in memory (the difference is caused by using `object` dtype for strings).  This dataset is small enough that you would normally use Pandas.\n",
    "\n",
    "We've chosen this size so that exercises finish quickly.  Dask.dataframe only really becomes meaningful for problems significantly larger than this, when Pandas breaks with the dreaded \n",
    "\n",
    "    MemoryError:  ...\n",
    "    \n",
    "#### Speed\n",
    "\n",
    "Dask.dataframe operations use `pandas` operations internally.  Generally they run at about the same speed except in the following two cases:\n",
    "\n",
    "1.  Dask introduces a bit of overhead, around 1ms per task.  This is usually negligible.\n",
    "2.  When Pandas releases the GIL (coming to `groupby` in the next version) `dask.dataframe` can call several pandas operations in parallel increasing speed somewhat proportional to the number of cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Recall and use Pandas API\n",
    "----------------------------------------\n",
    "\n",
    "If you are already familiar with the Pandas API then you should have a firm grasp on how to use `dask.dataframe`.  There are a couple of small changes.\n",
    "\n",
    "As noted above, computations on dask `DataFrame` objects don't perform work, instead they build up a dask graph.  We can evaluate this dask graph at any time using the `.compute()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=float64>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = df.amount.mean()  # create lazily evaluated result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934.0851584"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.compute()           # perform actual computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>: Using groupby(), calculate the average age for each sex.\n",
    "\n",
    "Try the following exercises\n",
    "\n",
    "<ol>\n",
    "<li> Use the `head()` method to get the first ten rows. </li> \n",
    "<li> Use the `drop_duplicates()` method to find all of the distinct names</li>\n",
    "<li> Use selections `df[...]` to find how many positive and negative amounts there are</li>\n",
    "<li> Use groupby `df.groupby(df.A).B.func()` to get the average amount per user ID</li>\n",
    "<li> Sort the result to (4) by amount, find the names of the top 10 </li>\n",
    "</ol>\n",
    "\n",
    "<p> This section should be easy if you are familiar with Pandas.  If you aren't then that's ok too.  You may find the [pandas documenation](http://pandas.pydata.org/) a useful read in the future.  Don't worry, future sections in this tutorial will not depend on this knowledge. </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Use the `head()` method to get the first ten rows\n",
    "#    Note, head computes by default, this is the only operation that doesn't need an explicit call to .compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Use the `drop_duplicates()` method to find all of the distinct names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Use selections `df[...]` to find how many positive and negative amounts there are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Use selections `df[...]` to find how many positive and negative amounts there are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Use groupby `df.groupby(df.A).B.func()` to get the average amount per user ID \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. Combine your answers to 3 and 4 to compute the average withdrawal (negative amount) per name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "clear_cell": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Use the `head()` method to get the first ten rows\n",
    "df.head()\n",
    "\n",
    "# 2. Use the `drop_duplicates()` method to find all of the distinct names\n",
    "df.names.drop_duplicates().compute()\n",
    "\n",
    "# 3. Use selections `df[...]` to find how many positive and negative amounts\n",
    "# there are\n",
    "len(df[df.amount < 0])\n",
    "\n",
    "# 3. Use selections `df[...]` to find how many positive and negative amounts\n",
    "# there are\n",
    "len(df[df.amount > 0])\n",
    "\n",
    "# 4. Use groupby `df.groupby(df.A).B.func()` to get the average amount per user\n",
    "# ID\n",
    "df.groupby(df.names).amount.mean().compute()\n",
    "\n",
    "# 5. Combine your answers to 3 and 4 to compute the average withdrawal\n",
    "# (negative amount) per name\n",
    "df2 = df[df.amount < 0]\n",
    "df2.groupby(df2.names).amount.mean().compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--    <img src=\"img/dask-dataframe.svg\" align=\"right\" width=\"40%\"> -->\n",
    "\n",
    "Divisions and the Index\n",
    "---------------------------\n",
    "\n",
    "The Pandas index associates a value to each record/row of your data.  Operations that align with the index, like `loc` can be a bit faster as a result.\n",
    "\n",
    "In `dask.dataframe` this index becomes even more important.  Recall that one dask `DataFrame` consists of several Pandas `DataFrame`s.  These dataframes are separated along the index by value.  For example, when working with time series we may partition our large dataset by month.\n",
    "\n",
    "Recall that these many partitions of our data may not all live in memory at the same time, instead they might live on disk; we simply have tasks that can materialize these pandas `DataFrames` on demand.\n",
    "\n",
    "Partitioning your data can greatly improve efficiency.  Operations like `loc`, `groupby`, and `merge/join` along the index are *much more efficient* than operations along other columns.  You can see how your dataset is partitioned with the `.divisions` attribute.  Note that data that comes out of simple data sources like CSV files aren't intelligently indexed by default.  In these cases the values for `.divisions` will be `None.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None, None, None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv(filename)\n",
    "df.divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.DataFrame<from-de..., npartitions=6>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we set the index to some new column then dask will divide our data roughly evenly along that column and create new divisions for us.  Warning, `set_index` triggers immediate computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Alice', 'Bob', 'Frank', 'Kevin', 'Quinn', 'Ursula', 'Zelda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.set_index('names')\n",
    "df2.divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>names</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>208</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>124</td>\n",
       "      <td>1877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>215</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>436</td>\n",
       "      <td>3610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>442</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  amount\n",
       "names             \n",
       "Alice  208     289\n",
       "Alice  124    1877\n",
       "Alice  215     217\n",
       "Alice  436    3610\n",
       "Alice  442     163"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the minimum and maximum values (\"Alice\" and \"Zelda\") as well as several intermediate values that separate our data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>names</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>303</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>445</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>204</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>53</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>204</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  amount\n",
       "names             \n",
       "Alice  303     216\n",
       "Alice  445     295\n",
       "Alice  204      -5\n",
       "Alice   53      32\n",
       "Alice  204      24"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations like `loc` only need to load the relevant partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.DataFrame<loc-ae3..., npartitions=1, divisions=('Edith', 'Edith')>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc['Edith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>names</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>396</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>195</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>40</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>408</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>210</td>\n",
       "      <td>2409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>444</td>\n",
       "      <td>2934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>40</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>195</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>245</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>210</td>\n",
       "      <td>2495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>210</td>\n",
       "      <td>2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>396</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>245</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>162</td>\n",
       "      <td>-291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>27</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>489</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>68</td>\n",
       "      <td>-1818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>329</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>329</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>40</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>489</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>27</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>-218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>-56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>210</td>\n",
       "      <td>2351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>-363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>40</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>195</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>195</td>\n",
       "      <td>2035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>329</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>444</td>\n",
       "      <td>3138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>177</td>\n",
       "      <td>-119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>329</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>245</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>329</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>100</td>\n",
       "      <td>-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>329</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>195</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>245</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>210</td>\n",
       "      <td>2348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>472</td>\n",
       "      <td>-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>210</td>\n",
       "      <td>2149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>245</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>210</td>\n",
       "      <td>2412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>195</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>40</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>195</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>396</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>489</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edith</th>\n",
       "      <td>27</td>\n",
       "      <td>2439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  amount\n",
       "names             \n",
       "Edith  472      92\n",
       "Edith  396      24\n",
       "Edith  195    2007\n",
       "Edith   40     453\n",
       "Edith  408     944\n",
       "Edith  210    2409\n",
       "Edith  444    2934\n",
       "Edith   40     502\n",
       "Edith  195    1946\n",
       "Edith  245      -3\n",
       "Edith  210    2495\n",
       "Edith  210    2383\n",
       "Edith  472     343\n",
       "Edith  396      17\n",
       "Edith  472     171\n",
       "Edith  245      -2\n",
       "Edith  100       0\n",
       "Edith  162    -291\n",
       "Edith   27    1952\n",
       "Edith  489     480\n",
       "Edith   68   -1818\n",
       "Edith  329      19\n",
       "Edith  329      42\n",
       "Edith   40     423\n",
       "Edith  489     416\n",
       "Edith   27    2125\n",
       "Edith  472    -218\n",
       "Edith  472     -56\n",
       "Edith  210    2351\n",
       "Edith  472     128\n",
       "...    ...     ...\n",
       "Edith  472    -363\n",
       "Edith   40     418\n",
       "Edith  195    1941\n",
       "Edith  195    2035\n",
       "Edith  329       5\n",
       "Edith  444    3138\n",
       "Edith  177    -119\n",
       "Edith  329      41\n",
       "Edith  245      -2\n",
       "Edith  472     273\n",
       "Edith  329      25\n",
       "Edith  472      47\n",
       "Edith  100     -16\n",
       "Edith  472      71\n",
       "Edith  329      53\n",
       "Edith  472     295\n",
       "Edith  195    1990\n",
       "Edith  245      -2\n",
       "Edith  472     317\n",
       "Edith  210    2348\n",
       "Edith  472     -85\n",
       "Edith  210    2149\n",
       "Edith  245      -3\n",
       "Edith  210    2412\n",
       "Edith  195    1992\n",
       "Edith   40     428\n",
       "Edith  195    1912\n",
       "Edith  396      13\n",
       "Edith  489     447\n",
       "Edith   27    2439\n",
       "\n",
       "[530080 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc['Edith'].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Storage\n",
    "\n",
    "\n",
    "Efficient storage can dramatically improve performance, particularly when operating repeatedly from disk.\n",
    "\n",
    "Decompressing text and parsing CSV files is expensive.  One of the most effective strategies with medium data is to use a binary storage format like HDF5.  Often the performance gains from doing this is sufficient so that you can switch back to using Pandas again instead of using `dask.dataframe`.\n",
    "\n",
    "In this section we'll learn how to efficiently arrange and store your datasets in on-disk binary formats.  We'll use the following:\n",
    "\n",
    "1.  [Pandas `HDFStore`](http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5) format on top of `HDF5`\n",
    "2.  Categoricals for storing text data numerically\n",
    "\n",
    "**Main Take-aways**\n",
    "\n",
    "1.  Storage formats affect performance by an order of magnitude\n",
    "2.  Text data will keep even a fast format like HDF5 slow\n",
    "3.  A combination of binary formats, column storage, and partitioned data turns one second wait times into 80ms wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to HDF5\n",
    "\n",
    "Pandas contains a specialized HDF5 format, `HDFStore`.  The ``dd.DataFrame.to_hdf`` method works exactly like the ``pd.DataFrame.to_hdf`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/accounts.h5'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = os.path.join('data', 'accounts.h5')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 1 s, total: 20.7 s\n",
      "Wall time: 17.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.to_hdf(target, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>names</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>Yvonne</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427</td>\n",
       "      <td>Ray</td>\n",
       "      <td>-767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>Quinn</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87</td>\n",
       "      <td>Victor</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>Dan</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   names  amount\n",
       "0   57  Yvonne     631\n",
       "1  427     Ray    -767\n",
       "2   91   Quinn     969\n",
       "3   87  Victor     267\n",
       "4  400     Dan     628"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = dd.read_hdf(target, '/data')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CSV to HDF5 speeds\n",
    "\n",
    "We do a simple computation that requires reading a column of our dataset and compare performance between CSV files and our newly created HDF5 file.  Which do you expect to be faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.06 s, sys: 624 ms, total: 5.68 s\n",
      "Wall time: 2.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14011277376"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 540 ms, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14011277376"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df2.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly they are about the same.  \n",
    "\n",
    "The culprit here is `names` column, which is of `object` dtype and thus hard to store efficiently.  There are two problems here:\n",
    "\n",
    "1.  How do we store text data like `names` efficiently on disk?\n",
    "2.  Why did we have to read the `names` column when all we wanted was `amount`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Store text efficiently with categoricals\n",
    "\n",
    "We can use Pandas categoricals to replace our object dtypes with a numerical representation.  This takes a bit more time up front, but results in better performance.\n",
    "\n",
    "More on categoricals at the [pandas docs](http://pandas.pydata.org/pandas-docs/stable/categorical.html) and [this blogpost](http://matthewrocklin.com/blog/work/2015/06/18/Categoricals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 1.83 s, total: 20.7 s\n",
      "Wall time: 13.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorize data, then store in HDFStore\n",
    "%time df.categorize(columns=['names']).to_hdf(target, '/data2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>names</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>Yvonne</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427</td>\n",
       "      <td>Ray</td>\n",
       "      <td>-767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>Quinn</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87</td>\n",
       "      <td>Victor</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>Dan</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   names  amount\n",
       "0   57  Yvonne     631\n",
       "1  427     Ray    -767\n",
       "2   91   Quinn     969\n",
       "3   87  Victor     267\n",
       "4  400     Dan     628"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It looks the same\n",
    "df2 = dd.read_hdf(target, '/data2')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 872 ms, sys: 104 ms, total: 976 ms\n",
      "Wall time: 959 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14011277376"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But loads more quickly\n",
    "%time df2.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly faster.  This tells us that it's not only the file type that we use but also how we represent our variables that influences storage performance.\n",
    "\n",
    "However this can still be better.  We had to read all of the columns (`names` and `amount`) in order to compute the sum of one (`amount`). For this, we could use other columnar on-disk stores like parquet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations\n",
    "-------------\n",
    "\n",
    "### What doesn't work?\n",
    "\n",
    "Dask.dataframe only covers a small but well-used portion of the Pandas API.\n",
    "This limitation is for two reasons:\n",
    "\n",
    "1.  The Pandas API is *huge*\n",
    "2.  Some operations are genuinely hard to do in parallel (e.g. sort)\n",
    "\n",
    "Additionally, some important operations like ``set_index`` work, but are slower\n",
    "than in Pandas because they may write out to disk.\n",
    "\n",
    "Finally, `dask.dataframe` is quite new and non-trivial bugs are frequently reported (and quickly fixed).\n",
    "\n",
    "### What definitely works?\n",
    "\n",
    "* Trivially parallelizable operations (fast):\n",
    "    *  Elementwise operations:  ``df.x + df.y``\n",
    "    *  Row-wise selections:  ``df[df.x > 0]``\n",
    "    *  Loc:  ``df.loc[4.0:10.5]``\n",
    "    *  Common aggregations:  ``df.x.max()``\n",
    "    *  Is in:  ``df[df.x.isin([1, 2, 3])]``\n",
    "    *  Datetime/string accessors:  ``df.timestamp.month``\n",
    "* Cleverly parallelizable operations (also fast):\n",
    "    *  groupby-aggregate (with common aggregations): ``df.groupby(df.x).y.max()``\n",
    "    *  value_counts:  ``df.x.value_counts``\n",
    "    *  Drop duplicates:  ``df.x.drop_duplicates()``\n",
    "    *  Join on index:  ``dd.merge(df1, df2, left_index=True, right_index=True)``\n",
    "* Operations requiring a shuffle (slow-ish, unless on index)\n",
    "    *  Set index:  ``df.set_index(df.x)``\n",
    "    *  groupby-apply (with anything):  ``df.groupby(df.x).apply(myfunc)``\n",
    "    *  Join not on the index:  ``pd.merge(df1, df2, on='name')``\n",
    "* Ingest operations\n",
    "    *  CSVs: ``dd.read_csv``\n",
    "    *  Pandas: ``dd.from_pandas``\n",
    "    *  Anything supporting numpy slicing: ``dd.from_array``\n",
    "    *  Dask.bag: ``mybag.to_dataframe(columns=[...])``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Nbtutor - export exercises",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
