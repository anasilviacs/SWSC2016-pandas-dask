{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/embarrassing.gif\" width=40% align=\"right\">\n",
    "\n",
    "Understanding Dask\n",
    "===========\n",
    "\n",
    "*Dask is a way to represent computations as dictionaries, and then analyze and execute them.*\n",
    "\n",
    "Dask supports parallel computing.  Internally it executes graphs of tasks with data dependencies.  In this section we talk about what these graphs look like and how to construct them.  We finish with exercises manually building graphs that use basic Pandas functionality.  This is straightforward but somewhat tedious.  We'll automate it in future sections.\n",
    "\n",
    "You can safely skip this section if you don't care about how dask works internally.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "*  [Dask graph specification](http://dask.pydata.org/en/latest/spec.html)\n",
    "*  [Discussion on custom graphs](http://dask.pydata.org/en/latest/custom-graphs.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask task graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Programming\n",
    "\n",
    "Normally we write functions and then use those function in linear code.  The Python interpreter executes this code from the top down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call functions in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = inc(a)\n",
    "\n",
    "x = 10\n",
    "y = inc(x)\n",
    "\n",
    "z = add(b, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though some of this work could have happened in parallel, Python went ahead and executed one line after the other sequentially.\n",
    "\n",
    "If we want to execute code in parallel then we need to stop Python from taking control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation as a data structure\n",
    "\n",
    "Instead of writing normal code we store the stages of the computation above as a Python dictionary where each function call becomes a Python tuple.\n",
    "\n",
    "This is going to look a little strange but we'll have the entire computation stored in a Python data structure that we can manipulate with *other* Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsk = {'a': 1, \n",
    "       'b': (inc, 'a'),\n",
    "       \n",
    "       'x': 10,\n",
    "       'y': (inc, 'x'),\n",
    "       \n",
    "       'z': (add, 'b', 'y')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(dsk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call a dictionary that looks like this a *dask graph*.  A dask graph is just a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Approaches to Delayed Evaluation\n",
    "\n",
    "Representing Python functions as tuples containing function names and arguments may seem strange, but in reality you are already familiar with the style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes we defer compuations with a lambda\n",
    "\n",
    "x = 15\n",
    "y = 30\n",
    "z = lambda: x + y\n",
    "\n",
    "# z delays the execution of x + y until we call z()\n",
    "# This is very similar to (add, 'x', 'y')\n",
    "\n",
    "# Sometimes we defer computations with strings\n",
    "\n",
    "z = \"x + y\"\n",
    "eval(z)\n",
    "\n",
    "# The variable 'z' stores a string that is a valid Python statement\n",
    "# We call eval to fully evaluate `z' and obtain the answer.\n",
    "\n",
    "# Sometimes we use functools.partial\n",
    "import functools\n",
    "z = functools.partial(add, x, y)\n",
    "\n",
    "# Dask delays evaluation with tuples\n",
    "z = (add, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask graph definition\n",
    "\n",
    "To be fully explicit, here is the definition of a dask graph taken from the [dask documentation](http://dask.pydata.org/en/latest/spec.html)\n",
    "\n",
    "A **dask graph** is a dictionary mapping data-keys to values or tasks.\n",
    "\n",
    "```python\n",
    "{'x': 1,\n",
    " 'y': 2,\n",
    " 'z': (add, 'x', 'y'),\n",
    " 'w': (sum, ['x', 'y', 'z'])}\n",
    "```\n",
    "\n",
    "A **key** can be any hashable value that is not a task.\n",
    "\n",
    "```python\n",
    "'x'\n",
    "('x', 2, 3)\n",
    "```\n",
    "\n",
    "A **task** is a tuple with a callable first element. Tasks represent atomic units of work meant to be run by a single worker.\n",
    "\n",
    "```python\n",
    "(add, 'x', 'y')\n",
    "```\n",
    "\n",
    "We represent a task as a `tuple` such that the *first element is a callable function* (like `add`), and the succeeding elements are *arguments* for that function.\n",
    "\n",
    "An **argument** to a task may be one of the following:\n",
    "\n",
    "1. Any key present in the dask like `'x'`\n",
    "2. Any other value like `1`, to be interpreted literally\n",
    "3. Other tasks like `(inc, 'x')`\n",
    "4. List of arguments, like `[1, 'x', (inc, 'x')]`\n",
    "\n",
    "So all of the following are valid tasks\n",
    "\n",
    "```python\n",
    "(add, 1, 2)\n",
    "(add, 'x', 2)\n",
    "(add, (inc, 'x'), 2)\n",
    "(sum, [1, 2])\n",
    "(sum, ['x', (inc, 'x')])\n",
    "(np.dot, np.array([...]), np.array([...]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute dask graphs\n",
    "\n",
    "The dask library contains functions to execute these dictionaries in parallel with multiple threads or multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.threaded import get\n",
    "get(dsk, 'z')  # Execute in multiple threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.multiprocessing import get\n",
    "get(dsk, 'z')  # Execute in multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as long as you're willing to write code in this funny way with dictionaries, dask will run your separate functions in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and Visualize Graphs\n",
    "\n",
    "Because our computation is just a dictionary we can write arbitrary functions to do a variety of useful analyses on these dictionaries.  A simple yet common operation is just to visualize the computation as a visual graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requires that you have pydot and graphviz installed\n",
    "# This isn't a problem if this doesn't work for you\n",
    "from dask.dot import dot_graph\n",
    "dot_graph(dsk, format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it\n",
    "----------\n",
    "\n",
    "The rest of this tutorial is fancy ways to construct and executing dask graphs.  We won't make any more by hand after this notebook.  If you'd like to learn more, read the [dask graph spec](http://dask.pydata.org/en/latest/spec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise - `read_csv`\n",
    "------------------------\n",
    "\n",
    "Let's look again at the exercise of the previous notebook with the accounts csv files. As an exercise we'll parallelize this example by rewriting it manually as a dask graph.  This will be a little tedious but should give us speedups right away.  In future sections we'll learn how dask submodules like `dask.dataframe` automate this work for us.\n",
    "\n",
    "There are three CSV files in your `data` directory.   We count how many rows are in all of these csv files total.  In normal Python we solve this problem in the following way (version without a for loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = ['accounts.%d.csv' % i for i in [0, 1, 2]]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "a = pd.read_csv(filenames[0])\n",
    "b = pd.read_csv(filenames[1])\n",
    "c = pd.read_csv(filenames[2])\n",
    "\n",
    "na = len(a)\n",
    "nb = len(b)\n",
    "nc = len(c)\n",
    "\n",
    "total = sum([na, nb, nc])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dask graph/dictionary for this computation\n",
    "\n",
    "Just as we turned code that looks like \n",
    "\n",
    "```python\n",
    "y = f(x)\n",
    "```\n",
    "\n",
    "into dictionaries like \n",
    "\n",
    "```python\n",
    "{'y': (f, 'x')}\n",
    "```\n",
    "\n",
    "We can transform the above calls to `pd.read_csv`, `len`, and `sum` into a dictionary of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsk = {'a': (pd.read_csv, filenames[0]),\n",
    "       'b': ...,\n",
    "       ...\n",
    "       'total': ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "clear_cell": true,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load solution-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute your dask graph\n",
    "\n",
    "We execute dask graphs with the `get` functions.  There is a get function for both multi-threading and multi-processing.  Get takes two arguments\n",
    "\n",
    "    get(dsk, output_key)\n",
    "\n",
    "Run the following cells and see how each get function performs.  Why is there a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.threaded import get\n",
    "%time get(dsk, 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.multiprocessing import get\n",
    "%time get(dsk, 'total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task graphs created by Dask collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we already saw how we could perform the above exercise with a `dask.dataframe` instead of manually creating the task dictionaries. Let's repeat that here and look at the created dask task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"accounts.*.csv\", blocksize=int(150e6)) # the large blocksize is to simplify the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Inspect dask graph\n",
    "\n",
    "Dask `DataFrame` copies a subset of the Pandas API.  \n",
    "\n",
    "However unlike Pandas, operations on dask.dataframes don't trigger immediate computation, instead they add key-value pairs to an underlying dask graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.visualize(format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.amount.sum().visualize(format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see graphs corresponding to a call to `dd.read_csv` and `df.amount.sum()` on the result.  \n",
    "\n",
    "Below we see the resulting computations as dictionaries.  You'll note that these dictionaries are a bit more complex than what we built by hand in the last section.  However if you look closely then you'll see all of the familiar elements of `pd.read_csv` and the filenames.\n",
    "\n",
    "Try changing around the expression `df.amount.sum()` and see how the dictionary and graph change.  Explore a bit with the Pandas syntax that you already know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dask  # .dask attribute contains underlying graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.amount.sum().dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building understanding by visualizing: `dask.diagnostics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides several tools to aid in profiling and inspecting dask graph execution. See the documentation for an overview (http://dask.pydata.org/en/latest/diagnostics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the default used cored to 4 for demonstration purposes\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import dask\n",
    "dask.set_options(pool=ThreadPool(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"accounts.*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the parallel computation of a grouped count operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = df.groupby('names')['amount'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the static task graph, we can also visualize the actual computations using dask's Profiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import Profiler, ResourceProfiler, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof:\n",
    "    result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize([prof, rprof])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the `Profiler` object show the execution time for each task as a rectangle, organized along the y-axis by worker (in this case threads). Similar tasks are grouped by color, and by hovering over each task one can see the key and task that each block represents.\n",
    "\n",
    "The results from the `ResourceProfiler object` show two lines, one for total CPU percentage used by all the workers, and one for total memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "------------\n",
    "\n",
    "We've learned about how dask graph represent computations and how we can execute these computations with dask schedulers / get functions.  We've made a few of these dictionaries by hand.  It's straightforward but perhaps tiresome. We then saw shortly how dask collections can generate similar dictionaries for us, and how we can visualize the execution of those graphs.\n",
    "\n",
    "In the next notebooks we will see more in depth examples using the Dask collections."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Nbtutor - export exercises",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "881px",
   "left": "0px",
   "right": "1585px",
   "top": "107px",
   "width": "335px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
